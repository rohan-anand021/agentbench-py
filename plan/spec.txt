# AgentBench: Offline Evaluation Harness for AI Coding Agents

## 0) Recommended Stack

**Language/Runtime**
- Python 3.11+ for the harness
- uv for dependency management
- ruff + black for formatting/linting
- mypy (optional, helpful once schemas stabilize)

**Core Libraries**
- CLI: typer + rich
- Config: pydantic-settings + pydantic
- HTTP (OpenRouter): httpx
- JSON: stdlib json (orjson if performance matters)
- Stats/reporting: numpy (bootstrap), statsmodels (McNemar, optional)

**Docker Integration**
- Start with docker CLI via subprocess (simpler, debuggable)
- Add optional docker SDK later if needed

**Git Integration**
- Use git CLI with subprocess
- Keep a local mirror cache of repos to avoid recloning

---

## 1) System Architecture

```
ab CLI
  → loads Suite → list of Tasks
  → for each Task:
      → RepoManager prepares checkout (pinned commit)
      → Sandbox creates containerized workspace
      → BaselineValidator runs failing_command (network policy configurable)
      → AgentRunner loops:
          → builds observation (failure logs + context)
          → calls LLM (OpenRouter) or scripted agent
          → executes tool calls in Sandbox
          → logs every event + file diff
      → Scorer runs passing_command, captures exit + summary
      → writes artifacts + JSONL attempt record
  → Report reads JSONL and emits summary + paired comparisons
```

**Key design choice**: Event-sourced logging. Every meaningful action becomes an event in JSONL, and the artifacts folder stores the raw evidence (stdout/stderr, diffs, selected files, etc.).

**Critical**: LLM calls happen on the host, not inside the sandbox container. This avoids leaking API keys, keeps sandbox network locked down, and makes artifacts easier to manage.

---

## 2) Repository Structure

```
agentbench-py/
  pyproject.toml
  README.md
  LICENSE
  .gitignore
  .python-version
  uv.lock
  .env.example

  agentbench/                     # Python package
    __init__.py
    cli.py                        # `ab` entrypoint (Typer)
    config.py                     # global config, env var loading
    constants.py

    schemas/
      task_spec.py                # Pydantic models for task.yaml
      run_record.py               # attempt record schema (JSONL)
      events.py                   # tool/agent events

    tasks/
      loader.py                   # load task folders → Task objects
      validator.py                # baseline fail-check logic

    repos/
      manager.py                  # git caching + checkout logic

    sandbox/
      docker_sandbox.py           # create/run containers, copy files
      limits.py                   # timeout, cpu/mem policies
      filesystem.py               # safe path utilities (no escape)

    tools/
      contract.py                 # ToolRequest/ToolResult types
      builtins.py                 # read/list/search/patch/run
      patching.py                 # apply unified diff safely

    agents/
      base.py                     # Agent interface
      scripted.py                 # Week 4 scripted agent
      llm_v0.py                   # Week 6 baseline LLM agent
      variants/
        context_packer.py         # Week 9
        subagents/                # Week 10
          router.py
          searcher.py
          patcher.py
          tester.py

    llm/
      client.py                   # provider-agnostic client
      openrouter.py               # OpenRouter implementation
      prompts/
        system_v1.txt
        formats.py                # message formatting + tool schema
      redaction.py                # logging policy (secrets)

    scoring/
      runner.py                   # run passing_command + parse results
      taxonomy.py                 # failure codes and mapping rules

    reporting/
      summary.py                  # Week 7
      paired.py                   # Week 11
      render.py                   # markdown/csv output helpers

    util/
      ids.py                      # run_id generation
      jsonl.py                    # append/read JSONL robustly
      time.py
      hashing.py                  # prompt/config hashes

  docker/
    py-runner/
      Dockerfile
      README.md

  configs/
    variants/
      baseline.yaml
      context_packer.yaml
      subagents.yaml

  tasks/                          # task definitions (data, not code)
    custom-dev/
      task_001_slug/
        task.yaml
        notes.md
      task_002_slug/
        task.yaml
    custom-heldout/
      task_101_slug/
        task.yaml

  artifacts/                      # generated output
    .gitkeep

  scripts/
    doctor.sh                     # environment checks (docker, etc.)
    smoke.sh                      # quick end-to-end sanity

  tests/                          # harness unit tests
    test_task_loader.py
    test_patch_apply.py
    test_artifacts_schema.py
```

---

## 3) Task Spec Schema (YAML)

```yaml
id: "pytest-bugfix-requests-parse"
suite: "custom-dev"

repo:
  url: "https://github.com/your-org/your-repo.git"
  commit: "a1b2c3d4e5f6..."

environment:
  docker_image: "ghcr.io/agentbench/py-runner:0.1.0"
  python: "3.11"
  workdir: "/workspace"
  network_policy: "setup_only"    # none | setup_only | always
  cpu_limit: 2
  mem_limit_mb: 4096
  timeout_sec: 900                # whole task budget
  tool_timeout_sec: 120           # per tool call command timeout

setup:
  commands:
    - "python -m pip install -U pip"
    - "pip install -r requirements.txt"
  capture:                        # commands to run after setup for logging
    - "python -V"
    - "pip -V"
    - "pip freeze"

validation:
  failing_command: "pytest -q"
  passing_command: "pytest -q"

agent:
  entrypoint: "llm_v0"            # or scripted
  max_steps: 30                   # tool-call budget
  allow_file_write: true
  editable_globs:                 # optional: paths to constrain edits
    - "**/*.py"
    - "pyproject.toml"

artifacts:
  keep_workspace: true            # store final repo state snapshot
  redact:
    - "OPENROUTER_API_KEY"
```

**Notes:**
- Keep setup separate from validation
- Network policy should default to none during agent steps
- `network_policy=setup_only`: run setup container with network, run agent/test container with `--network=none`

---

## 4) Artifact & Logging Spec

### Artifact Directory Layout (per suite run)

```
artifacts/
  runs/
    2025-12-15T18-00-00-0500__custom-dev__baseline__anthropic_claude-3-5/
      run.json                    # metadata about the suite run
      attempts.jsonl              # 1 line per task attempt summary
      events.jsonl                # event stream (tools, agent turns, etc.)
      report_summary.md           # Week 7 output

      tasks/
        pytest-bugfix-requests-parse/
          task.yaml               # copied spec used for the run (freeze it)
          repo/                   # optional snapshot of final state
          logs/
            setup_stdout.txt
            setup_stderr.txt
            failing_stdout.txt
            failing_stderr.txt
            passing_stdout.txt
            passing_stderr.txt
          agent/
            conversation.jsonl    # messages + tool calls (redacted)
            tool_calls.jsonl      # tool invocations (inputs/outputs)
          diffs/
            step_0001.patch
            step_0002.patch
          meta/
            timings.json
            environment.json      # docker image id, limits, etc.
```

### JSONL Attempt Record Schema

One summary line per task attempt:

```json
{
  "run_id": "01JFD...ULID",
  "task_id": "pytest-bugfix-requests-parse",
  "suite": "custom-dev",
  "variant": "baseline",
  "model": {
    "provider": "openrouter",
    "name": "anthropic/claude-3.5-sonnet",
    "temperature": 0.2,
    "top_p": 1.0,
    "max_tokens": 2048,
    "prompt_version": "system_v1@sha256:..."
  },
  "timestamps": {
    "started_at": "2025-12-15T18:05:12-0500",
    "ended_at": "2025-12-15T18:07:31-0500"
  },
  "duration_sec": 139.2,
  "baseline_validation": {
    "attempted": true,
    "failed_as_expected": true,
    "exit_code": 1
  },
  "result": {
    "passed": true,
    "exit_code": 0,
    "failure_reason": null
  },
  "limits": {
    "timeout_sec": 900,
    "tool_timeout_sec": 120
  },
  "artifact_paths": {
    "task_dir": "tasks/pytest-bugfix-requests-parse",
    "failing_stdout": "logs/failing_stdout.txt",
    "passing_stdout": "logs/passing_stdout.txt"
  }
}
```

### Events JSONL

Use events to reconstruct the run:
- `task_started`, `setup_command_started`, `setup_command_finished`
- `agent_turn_started`, `llm_request`, `llm_response` (redacted)
- `tool_call_started`, `tool_call_finished`
- `patch_applied`
- `tests_started`, `tests_finished`
- `task_finished`

---

## 5) Tool API Contract

A small, stable tool set:

1. **list_files**(root: str, glob: str | null)
2. **read_file**(path: str, start_line: int | null, end_line: int | null)
3. **search**(query: str, glob: str | null, max_results: int)
4. **apply_patch**(unified_diff: str)
5. **run**(command: str, timeout_sec: int | null, env: dict | null)

**Tool results should include:**
- ok: bool
- stdout, stderr, exit_code (for run)
- changed_files (for patch)
- error_type + error_message (structured)

**Constraints:**
- Enforce path safety (no `../` escapes)
- Optionally enforce `editable_globs`

---

## 6) Docker Sandboxing

### Container Policy

- Use a prebuilt runner image: `ghcr.io/agentbench/py-runner:<version>`
- Run as non-root user inside container
- Mount workspace read-write, but do not mount home dir
- Disable network during agent/test steps: `--network=none`
- Add resource limits: `--cpus=2`, `--memory=4g`
- Add timeouts at two levels: harness-level per command, container-level kill if needed

### Runner Image Contents

- python (3.11)
- git
- build essentials (common pip packages compile extensions)
- ripgrep (fast search)
- a non-root user

### Reproducibility

Record in artifacts:
- docker image tag and digest
- OS info from inside container
- pip freeze (optional but useful)

### Two-Phase Container Approach

1. **Setup phase** (optional network)
   - `docker run --network=bridge ...`
   - Runs `setup.commands`
   - Logs `pip freeze` after setup

2. **Agent + test phase** (no network)
   - `docker run --network=none ...`
   - Runs tool commands and test commands

Both containers mount the same host workspace directory.

### Caching

**Git:**
- Keep a bare mirror cache: `~/.cache/agentbench/git/<hash>.git`

**Pip:**
- Mount a persistent pip cache volume:
  - host: `~/.cache/agentbench/pip`
  - container: `/home/runner/.cache/pip`

---

## 7) OpenRouter Integration

### Configuration Sources (priority order)

1. CLI flags (`--model ... --temperature ...`)
2. Run config YAML (variant config)
3. Model preset registry (optional defaults)
4. Code defaults (last resort)

### What to Log

**Log:**
- model name string
- inference params
- prompt version hash
- token usage if available
- tool call decisions and outputs

**Redact:**
- API keys
- any detected secrets from repo files

---

## 8) CLI Design

```bash
# Run a suite with a variant + model
ab run \
  --suite custom-dev \
  --variant baseline \
  --model anthropic/claude-3.5-sonnet \
  --max-steps 30 \
  --workers 4 \
  --out artifacts/runs

# Run single task (fast iteration)
ab run-task --task tasks/custom-dev/.../task.yaml

# Validate tasks (baseline should fail)
ab validate-tasks --suite custom-dev --workers 4

# Generate summary report
ab report summary --run artifacts/runs/<run-dir>

# Paired comparison
ab report paired \
  --run-a artifacts/runs/<baseline-run> \
  --run-b artifacts/runs/<variant-run> \
  --method mcnemar

# Environment check
ab doctor
```

---

## 9) Failure Taxonomy

Define a small set of codes and map them deterministically:

| Code | Description |
|------|-------------|
| `SETUP_FAILED` | Dependency install failure, missing tools |
| `BASELINE_NOT_FAILING` | Invalid task (baseline passes) |
| `TIMEOUT` | Command exceeded budget |
| `SANDBOX_ERROR` | Docker issues, mount failure |
| `TOOL_ERROR` | Patch failed to apply, invalid path |
| `TESTS_FAILED` | pytest exit non-zero after agent |
| `AGENT_GAVE_UP` | Hit max steps without improvement |
| `LLM_ERROR` | Rate limit, API failure |

**Rule:** Every non-pass must have exactly one primary reason code.

---

## 10) Determinism Protocol

**Harness determinism:**
- same task spec + same commit + same docker image → same baseline validation result (modulo flaky tests)

**Model nondeterminism:**
- allow temperature control
- optionally run multiple seeds per task later
- keep paired comparisons consistent: same tasks, budgets, timeouts, harness version

**Version everything:**
- harness version (git SHA)
- prompt version hash
- model preset registry version

---

## 11) Weekly Plan

### Week 1: Skeleton + Docker Runner
- `scripts/doctor.sh` to verify docker, buildkit, disk space
- `ab run-task --task ... --out ...` command
- `Sandbox.run()` that launches container, runs command, writes stdout/stderr
- **Success:** artifacts include docker image digest + executed command list

### Week 2: Loader + Suite + Baseline Validation
- `tasks/loader.py`: enumerates `tasks/<suite>/*/task.yaml`
- `tasks/validator.py`: runs `failing_command` after setup
- Refuse tasks where baseline passes (mark invalid in JSONL)

### Week 3: Schema v0 + Taxonomy
- `schemas/run_record.py` + JSONL writer
- `scoring/taxonomy.py`: map exceptions/exit codes/timeouts → reason codes
- "Always write record even on crash" (try/finally)

### Week 4: Tool API + Scripted Agent
- `tools/builtins.py` + safe filesystem layer
- `agents/scripted.py`: hard-coded solve for toy task
- `events.jsonl` logs each tool invocation with timing

### Week 5: OpenRouter Client + Config Capture
- `llm/openrouter.py` using httpx
- `llm/prompts/system_v1.txt` + hashing
- `config.py`: load OpenRouter key from env; record params in artifacts

### Week 6: Baseline LLM Agent v0 + 10 Tasks
- `agents/llm_v0.py`:
  - Loop: run failing tests → summarize → pick files → patch → rerun
  - Stop conditions: pass, max steps, repeated failure, tool errors
- Ensure tool-call budget is enforced and logged

### Week 7: Reporting v0
- `reporting/summary.py` reading `attempts.jsonl`
- Output markdown table + failure histogram + "hardest tasks"
- Include artifact links/paths

### Week 8: Expand Benchmark + Heldout + Stability Checks
- `ab validate-tasks` that:
  - Runs baseline fail-check twice (detect flakiness)
  - Records "flaky" label if inconsistent
- Document task inclusion rules in `tasks/README.md`

### Week 9: Variant Framework + Context Packer
- `agents/base.py` with variant config structure
- `variants/context_packer.py`:
  - Parse pytest output for filenames
  - Read those files + referenced modules
  - Cap context by token budget heuristic
- Emit side-by-side comparison CSV (no stats yet)

### Week 10: Subagents
- `variants/subagents/router.py`: decides next action
- Specialists return structured outputs:
  - Searcher: list of candidate files + snippets
  - Patcher: proposed unified diff
  - Tester: recommendation based on test output
- Log delegation graph in events

### Week 11: Paired Comparisons + Regression Gating
- `reporting/paired.py`:
  - Paired contingency table
  - McNemar p-value or bootstrap CI for delta pass rate
- "Regression gate" config: max allowed heldout drop, or require non-inferiority

### Week 12: Polish + Docs + Write-up
- README.md sections: installation, running suites, adding tasks, reading artifacts
- Freeze a "v1" runner docker image tag (immutable)

---

## 12) Gotchas to Plan For

- **Test flakiness** will dominate your time. Baseline validation should include a flake check early.

- **Dependency installs** are a major cost. Pip cache mounting is non-negotiable for iteration speed.

- **Patch application** must be robust:
  - Reject patches that touch files outside workspace
  - Verify hunks apply; otherwise return a structured error

- **Log size control:**
  - Truncate very large stdout/stderr with "saved first N / last N lines" policy
  - Store full logs optionally
